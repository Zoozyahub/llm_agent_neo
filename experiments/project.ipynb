{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Итоговый проект DS\n",
    "\n",
    "* В качестве моделей для тестов выбраны LLama 3.2 и Mistral   \n",
    "mistral:latest     f974a74358d6    4.1 GB    12 seconds ago  \n",
    "llama3.2:latest    a80c4f17acd5    2.0 GB    3 weeks ago  \n",
    "* Для каждой модели будут проведены экмперементы с 2 эмбеддерами MiniLM-L12-v2 и mpnet-base-v2\n",
    "* База данных RubQ 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве эмбедеров не подойдут такие модели наприммер как word2vec, так-как\n",
    "* они не учитывают контекст и каждое слово всегда имеет одно значение \n",
    "* а также не умеют понимать предложение и текст полностью так как работают на уровне слов\n",
    "\n",
    "Преимущества SentenceTransformer\n",
    "\n",
    "* Хорошо учитвают контекст \n",
    "* Хорошо справляются с семантическим поиском"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\pypy\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedder_light = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "# этот эмбеддер показался мне куда хуже, так то временно он будет закомментирован  \n",
    "#embedder_heavy = SentenceTransformer(\"paraphrase-multilingual-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Mistral 7B\n",
    "model_mistral = ChatOpenAI(base_url=\"http://localhost:11434/v1\", api_key='1', model=\"mistral\")\n",
    "\n",
    "# LLaMA 3.2\n",
    "model_llama = ChatOpenAI(base_url=\"http://localhost:11434/v1\", api_key='1', model=\"llama3.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "splits = {'test': 'data/test-00000-of-00001-d519841742f463e6.parquet', 'dev': 'data/dev-00000-of-00001-d7e3040a344e1e68.parquet'}\n",
    "df = pd.read_parquet(\"hf://datasets/d0rj/RuBQ_2.0/\" + splits[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_text</th>\n",
       "      <th>answer_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Что может вызвать цунами?</td>\n",
       "      <td>Землетрясение</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Кто написал роман «Хижина дяди Тома»?</td>\n",
       "      <td>Г. Бичер-Стоу</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Кто автор пьесы «Ромео и Джульетта»?</td>\n",
       "      <td>Шекспир</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Как называется столица Румынии?</td>\n",
       "      <td>Бухарест</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>На каком инструменте играл Джимми Хендрикс?</td>\n",
       "      <td>Гитара</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 question_text    answer_text\n",
       "0                    Что может вызвать цунами?  Землетрясение\n",
       "1        Кто написал роман «Хижина дяди Тома»?  Г. Бичер-Стоу\n",
       "2         Кто автор пьесы «Ромео и Джульетта»?        Шекспир\n",
       "3              Как называется столица Румынии?       Бухарест\n",
       "4  На каком инструменте играл Джимми Хендрикс?         Гитара"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['question_text', 'answer_text']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chromadb import Client, Settings\n",
    "\n",
    "\n",
    "# Инициализация Chroma\n",
    "# client = Client(settings=Settings(persist_directory=\"./chroma_db\"))\n",
    "# collection = client.create_collection(name=\"rubq\")\n",
    "\n",
    "# Создание эмбеддингов и добавление в базу\n",
    "# for idx, row in df[['question_text', 'answer_text']].iterrows():\n",
    "#     question = row['question_text']\n",
    "#     answer = row['answer_text']\n",
    "#     embedding = embedder_heavy.encode(question)  # используем тяжелый эмбедер\n",
    "#     collection.add(ids=[str(idx)], embeddings=[embedding], documents=[answer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# запускать если надо создать коллекцию\n",
    "from chromadb import Client, Settings\n",
    "\n",
    "# Инициализация Chroma\n",
    "client = Client(settings=Settings(persist_directory=\"./chroma_db\"))\n",
    "# Создание новой коллекции для легкого эмбеддера\n",
    "collection_light = client.create_collection(name=\"rubq_light\")\n",
    "\n",
    "for idx, row in df[['question_text', 'answer_text']].iterrows():\n",
    "    question = row['question_text']\n",
    "    answer = row['answer_text']\n",
    "    embedding = embedder_light.encode(question)  # Используем легкий эмбеддер\n",
    "    \n",
    "    # Добавляем данные в коллекцию, включая метаданные (вопросы)\n",
    "    collection_light.add(\n",
    "        ids=[str(idx)], \n",
    "        embeddings=[embedding], \n",
    "        documents=[answer], \n",
    "        metadatas=[{\"question\": question}]  # Сохраняем вопрос как метаданные\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# если надо удалить\n",
    "client.delete_collection(name=\"rubq_light\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# если уже создана\n",
    "from chromadb import Client, Settings\n",
    "\n",
    "client = Client(settings=Settings(persist_directory=\"./chroma_db\"))\n",
    "collection_light = client.get_collection(name=\"rubq_light\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_from_db(collection, query, embedder, n_results=1):\n",
    "    # Генерируем эмбеддинг для пользовательского запроса\n",
    "    query_embedding = embedder.encode(query)\n",
    "    \n",
    "    # Выполняем запрос в ChromaDB\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=n_results\n",
    "    )\n",
    "    \n",
    "    # Получаем документы, метаданные (в данном случае это вопросы), и соответствующие идентификаторы\n",
    "    documents = results.get('documents', [])  # Список ответов\n",
    "    metadatas = results.get('metadatas', [])  # Список вопросов\n",
    "    ids = results.get('ids', [])              # Список идентификаторов\n",
    "    \n",
    "    # Формируем список вопрос-ответ пар\n",
    "    qa_pairs = [\n",
    "        {\"id\": doc_id, \"question\": meta, \"answer\": doc}\n",
    "        for doc_id, meta, doc in zip(ids, metadatas, documents)\n",
    "    ]\n",
    "    \n",
    "    res = {}\n",
    "    for item in qa_pairs:\n",
    "        res[\"Вопрос:\"] =  item['question'][0]['question']\n",
    "        res[\"Ответ:\"] = item['answer'][0]\n",
    "    \n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = retrieve_from_db(collection, \"Кто автор Хижина дяди Тома\", embedder_heavy)\n",
    "# for doc in results:\n",
    "#     print(doc)\n",
    "\n",
    "# тут вывод \n",
    "# совсем не соответсвет контексту \n",
    "# топ1 по посику в бд - Шут а нужно Г. Бичер-Стоу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Вопрос:': 'На каком инструменте играл Джимми Хендрикс?', 'Ответ:': 'Гитара'}\n"
     ]
    }
   ],
   "source": [
    "query = \"играл Джимми Хендрикс?\"\n",
    "results = retrieve_from_db(collection_light, query, embedder_light, n_results=1)\n",
    "\n",
    "print(results)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchRun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Если кактусу в течение долгого времени не хватает воды, он «молчать» не будет и расскажет вам о жажде с помощью следующих изменений: А если создать кактусу все условия, ... Для этого подойдёт удобрение с высоким содержанием калия (например, для томатов) или специальное удобрение для кактусов. Подкармливайте кактус раз в ... Удобрение кактусов ... какое освещение подходит кактусам и как обеспечить его правильный баланс. 1.1. Естественное освещение ... что позволяет кактусу аккуратно взять столько воды, сколько ... Как ухаживать за кактусом: полив, пересадка и болезни Чем подкормить кактусы в домашних условиях Причины, по которым кактус не цветет Как подобрать почву Удобрение кактусов Чрезмерное удобрение. В природе кактусы предпочитают песчаные, бедные гумусом почвы, содержащие мало питательных веществ. Это не значит, что растение вообще может обходиться без какого ...\n"
     ]
    }
   ],
   "source": [
    "def search_duckduckgo(query): \n",
    "    tool = DuckDuckGoSearchRun(max_results=3)\n",
    "    results = tool.invoke(query)\n",
    "    return results\n",
    "\n",
    "print(search_duckduckgo('Какое удобрение подойдёт кактусу?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated, Any\n",
    "import operator\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, ToolMessage\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "_ = load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Регулярное выражение для парсинга действий\n",
    "action_re = re.compile('^Action: (\\w+): (.*)$')\n",
    "\n",
    "# # Определение структуры состояния агента\n",
    "# class AgentState(TypedDict):\n",
    "#     messages: Annotated[list[Any], operator.add]\n",
    "#     memory: list[str]  # Память для хранения прошлых сообщений\n",
    "    \n",
    "    \n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[Any], operator.add]  # Сообщения (вопросы и ответы)\n",
    "    memory: list[str]  # Память (история диалога)\n",
    "    db_results: dict  # Результаты поиска в БД\n",
    "    web_results: str  # Результаты поиска в интернете\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, model, collection, embedder, system=\"\"):\n",
    "        self.model = model\n",
    "        self.collection = collection\n",
    "        self.embedder = embedder\n",
    "        self.system = system\n",
    "        self.graph = self._build_graph()\n",
    "\n",
    "    def _build_graph(self):\n",
    "        graph = StateGraph(AgentState)\n",
    "\n",
    "        # Добавляем узлы\n",
    "        graph.add_node(\"refine_query\", self.refine_query)  # Новый узел\n",
    "        graph.add_node(\"search_db\", self.search_db)\n",
    "        graph.add_node(\"evaluate_relevance\", self.evaluate_relevance)\n",
    "        graph.add_node(\"search_web\", self.search_web)\n",
    "        graph.add_node(\"edit_web_response\", self.edit_web_response)\n",
    "\n",
    "        # Добавляем переходы\n",
    "        graph.add_edge(\"refine_query\", \"search_db\")  # После уточнения ищем в БД\n",
    "        graph.add_edge(\"search_db\", \"evaluate_relevance\")\n",
    "        graph.add_conditional_edges(\n",
    "            \"evaluate_relevance\",\n",
    "            self.decide_next_step,\n",
    "            {\"end\": END, \"search_web\": \"search_web\"}\n",
    "        )\n",
    "        graph.add_edge(\"search_web\", \"edit_web_response\")\n",
    "        graph.add_edge(\"edit_web_response\", END)\n",
    "\n",
    "        # Устанавливаем начальный узел\n",
    "        graph.set_entry_point(\"refine_query\")\n",
    "\n",
    "        return graph.compile()\n",
    "    \n",
    "    def refine_query(self, state: AgentState):\n",
    "        \"\"\"\n",
    "        Уточняет запрос пользователя на основе контекста.\n",
    "        \"\"\"\n",
    "        user_query = state[\"messages\"][-1][\"content\"]\n",
    "        context = \"\\n\".join(state[\"memory\"])  # История диалога\n",
    "        \n",
    "        if 'По поиску в RuBQ Ответ:' in context:\n",
    "            context = context.replace('По поиску в RuBQ Ответ:', '')\n",
    "        elif 'По поиску в DuckDuckGo:' in context:\n",
    "            context = context.replace('По поиску в DuckDuckGo:') \n",
    "        # print('----')\n",
    "        # print('Что в истории до редактирования', context )\n",
    "        # print('----')\n",
    "        \n",
    "        REFINE_QUERY_PROMPT = \"\"\"\n",
    "            Ты ассистент для уточнения запросов пользователя.\n",
    "                ### Твоя задача ###\n",
    "                На основе предоставленной истории и вопроса пользователя составь уточнённый запрос, который будет максимально полным и самодостаточным. Если запрос уже ясен и не требует контекста, оставь его без изменений. Если запрос относится к новой теме, просто перепиши его как есть.\n",
    "\n",
    "                ### Примеры ###\n",
    "                История: \"Кактусам подойдут удобрения с высоким содержанием калия.\"\n",
    "                Запрос: \"А сколько раз поливать?\"\n",
    "                Твоя корректировка: \"Сколько раз поливать кактусы?\"\n",
    "\n",
    "                История: \"Лучший язык для начинающих программистов — Python.\"\n",
    "                Запрос: \"Что насчет Java?\"\n",
    "                Твоя корректировка: \"Подходит ли Java для начинающих программистов?\"\n",
    "\n",
    "                История: \n",
    "                Запрос: \"Какова площадь Луны?\"\n",
    "                Твоя корректировка: \"Какова площадь Луны?\"\n",
    "\n",
    "                История: \"Кактусам подойдут удобрения с высоким содержанием калия.\"\n",
    "                Запрос: \"Расскажи про животных.\"\n",
    "                Твоя корректировка: \"Расскажи про животных.\"\n",
    "\n",
    "                ### История ###\n",
    "                {context}\n",
    "\n",
    "                ### Запрос пользователя ###\n",
    "                {human_message}\n",
    "\n",
    "                ### Твоя корректировка ###\n",
    "                \"\"\"\n",
    "\n",
    "        # Формируем промпт для уточнения запроса\n",
    "        prompt = REFINE_QUERY_PROMPT.format(\n",
    "            context=context,\n",
    "            human_message=user_query\n",
    "        )\n",
    "\n",
    "        # Запрашиваем уточнение у LLM\n",
    "        response = self.model.invoke(prompt)\n",
    "        refined_query = response.content.strip()\n",
    "\n",
    "        # Сохраняем уточнённый запрос в состоянии\n",
    "        print('Отредактированый вопрос:', refined_query)\n",
    "        state[\"messages\"][-1][\"content\"] = refined_query\n",
    "        return state\n",
    "\n",
    "    def search_db(self, state: AgentState):\n",
    "        \"\"\"\n",
    "        Ищет в базе данных и сохраняет результаты в состоянии.\n",
    "        \"\"\"\n",
    "        user_query = state[\"messages\"][-1][\"content\"]\n",
    "        db_results = retrieve_from_db(self.collection, user_query, self.embedder)\n",
    "        state[\"db_results\"] = db_results\n",
    "        return state\n",
    "\n",
    "    def evaluate_relevance(self, state: AgentState):\n",
    "        \"\"\"\n",
    "        Оценивает релевантность ответа из БД с помощью LLM.\n",
    "        \"\"\"\n",
    "        user_query = state[\"messages\"][-1][\"content\"]\n",
    "        db_question = state[\"db_results\"][\"Вопрос:\"]\n",
    "        db_answer = state[\"db_results\"][\"Ответ:\"]\n",
    "\n",
    "        RELEVANCE_PROMPT = \"\"\"\n",
    "                Ты ассистент сравнения вопросов по семантической близости!\n",
    "                Можно ли ответить на эти 2 вопроса одинаково правильно(Одинаковы ли они по смыслу)?\n",
    "                1. {user_query}\n",
    "                2. {db_question}\n",
    "\n",
    "                Верни только \"да\" или \"нет\" и ничего кроме этого!\n",
    "                \"\"\"\n",
    "        # Формируем промпт для оценки релевантности\n",
    "        prompt = RELEVANCE_PROMPT.format(\n",
    "            user_query=user_query,\n",
    "            db_question=db_question,\n",
    "            db_answer=db_answer\n",
    "        )\n",
    "\n",
    "        # Запрашиваем оценку у LLM\n",
    "        response = self.model.invoke(prompt)\n",
    "        # print(f'Вопрос бд:{db_question}, Ответ бд:{db_answer}')\n",
    "        # print(f\"Оценка релевантности: {response.content}\")  # Отладочный вывод\n",
    "\n",
    "        # Если ответ релевантен, добавляем его в память\n",
    "        if \"да\" in response.content.lower():\n",
    "            # state[\"memory\"].append(f\"По поиску в RuBQ Ответ: {db_answer}\")\n",
    "            self.update_memory(state, user_query, f\"По поиску в RuBQ Ответ: {db_answer}\")\n",
    "            state[\"next_step\"] = \"end\"  # Завершаем выполнение\n",
    "        else:\n",
    "            state[\"next_step\"] = \"search_web\"  # Переходим к поиску в интернете\n",
    "\n",
    "        return state\n",
    "\n",
    "    def search_web(self, state: AgentState):\n",
    "        \"\"\"\n",
    "        Ищет в интернете с помощью DuckDuckGo.\n",
    "        \"\"\"\n",
    "        user_query = state[\"messages\"][-1][\"content\"]\n",
    "        web_results = self.search_duckduckgo(user_query)\n",
    "        state[\"web_results\"] = web_results\n",
    "        return state\n",
    "\n",
    "    def edit_web_response(self, state: AgentState):\n",
    "        \"\"\"\n",
    "        Редактирует ответ из интернета с помощью LLM.\n",
    "        \"\"\"\n",
    "        user_query = state[\"messages\"][-1][\"content\"]\n",
    "        web_results = state[\"web_results\"]\n",
    "\n",
    "        # Формируем промпт для редактирования\n",
    "        prompt = f\"\"\"\n",
    "        Ты получил ответ из интернета на вопрос пользователя. Отредактируй его, чтобы он был понятным и удобным для чтения.\n",
    "\n",
    "        Вопрос пользователя: {user_query}\n",
    "        Ответ из интернета: {web_results}\n",
    "\n",
    "        Отредактированный ответ:\n",
    "        \"\"\"\n",
    "\n",
    "        # Запрашиваем редактирование у LLM\n",
    "        response = self.model.invoke(prompt)\n",
    "\n",
    "        # Добавляем отредактированный ответ в память\n",
    "        # state[\"memory\"].append(f\"По поиску в DuckDuckGo: {response.content}\")\n",
    "        self.update_memory(state, user_query, f\"По поиску в DuckDuckGo: {response.content}\")\n",
    "        return state\n",
    "\n",
    "    def decide_next_step(self, state: AgentState):\n",
    "        \"\"\"\n",
    "        Определяет следующий шаг на основе состояния.\n",
    "        \"\"\"\n",
    "        return state.get(\"next_step\", \"search_web\")\n",
    "\n",
    "    def search_duckduckgo(self, query):\n",
    "        \"\"\"\n",
    "        Ищет в интернете с помощью DuckDuckGo.\n",
    "        \"\"\"\n",
    "        tool = DuckDuckGoSearchRun(max_results=1)\n",
    "        return tool.invoke(query)\n",
    "\n",
    "    def update_memory(self, state: AgentState, user_message, agent_message):\n",
    "        \"\"\"\n",
    "        Обновляет память агента.\n",
    "        \"\"\"\n",
    "        state[\"memory\"].append(f\"User: {user_message}\")\n",
    "        state[\"memory\"].append(f\"Agent: {agent_message}\")\n",
    "\n",
    "    def invoke(self, state: AgentState):\n",
    "        \"\"\"\n",
    "        Запускает агента с начальным состоянием.\n",
    "        \"\"\"\n",
    "        return self.graph.invoke(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вопрос бд:На каком инструменте играл Джимми Хендрикс?, Ответ бд:Гитара\n",
      "Оценка релевантности:  Да\n",
      "По поиску в RuBQ Ответ: Гитара\n",
      "Вопрос бд:Какой пост занимает Шойгу?, Ответ бд:Губернатор Московской области\n",
      "Оценка релевантности:  Нет\n",
      "По поиску в DuckDuckGo:  Удобрения для кактусов в домашних условиях: Чем подкармливать растение?\n",
      "\n",
      "   Для здорового роста кактуса, можно применять следующие типы удобрений:\n",
      "   1. Удобрение с высоким содержанием калия (например, для томатов) или специальное удобрение для кактусов.\n",
      "\n",
      "   Подкармливайте кактус раз в 2-3 недели во время активного роста и вегетации. Следуя этой рекомендации поможет избежать нарушения равновесия солей в почве и переобогащения, что может приводить к отравлению кактуса.\n",
      "\n",
      "   Какой грунт выбрать для кактуса?\n",
      "   1. Полиферт предлагает комплексное минеральное водорастворимое удобрение с микроэлементами в хелатной форме с различным соотношением фосфора, калия и азота.\n",
      "   2. Лучшая стратегия для кактуса - обильный полив и очень хороший дренаж, который позволит быстро вывести излишки воды и не допустит гниения корней.\n",
      "\n",
      "   Как ухаживать за кактусом:\n",
      "   1. Полив - придерживайтесь правильного полива, чтобы не было ни заболазнивания, ни просыхания почвы. Используйте нюансы полива в соответствии с временем года и потребностями растения.\n",
      "   2. Пересадка - пересаживать кактус можно не чаще одного раза в год, но лучше редко, только когда почва станет плохой или корни вырастут из верхней части горшка.\n",
      "   3. Болезни - для предотвращения болезней, следите за тем, чтобы почва была хорошо дренирована и не было слишком много влаги или солёности в почве. Если кактус уже страдает от болезни, обратитесь к специалисту или изучите информацию для конкретного вида кактуса.\n",
      "\n",
      "   Причины, по которым кактус не цветет:\n",
      "   1. Недостаточное освещение - необходимо обеспечить кактусу правильное количество и качество света для активного цветения.\n",
      "   2. Неправильный полив - заболазнивание или просыхание почвы могут приводить к отказу в цветении.\n",
      "   3. Отсутствие удобрений в почве - необходимо периодически подкармливать кактус, чтобы обеспечить достаточное количество микроэлементов в почве.\n",
      "   4. Неправильная температура - температура должна быть в рамках оптимального диапазона для конкретного вида кактуса, чтобы он приобрел красочность и начал цвести.\n"
     ]
    }
   ],
   "source": [
    "# Агент с Mistral и легким эмбеддером\n",
    "#agent_mistral = Agent(model_mistral, [tool], system=prompt, collection=collection_light, embedder=embedder_light)\n",
    "\n",
    "# Агент с LLaMA 2 и легким эмбеддером\n",
    "#agent_llama = Agent(model_llama, [tool], system=prompt,  collection=collection_light, embedder=embedder_light)\n",
    "\n",
    "# Инициализация модели и коллекции\n",
    "model = model_mistral\n",
    "collection = collection_light\n",
    "embedder = embedder_light  \n",
    "\n",
    "# Создание агента\n",
    "agent = Agent(model, collection, embedder)\n",
    "\n",
    "# Инициализация состояния\n",
    "state = {\n",
    "    \"messages\": [{\"content\": \"На чем играл Джимми Хендрикс?\"}],\n",
    "    \"memory\": [],\n",
    "    \"db_results\": None,\n",
    "    \"web_results\": None\n",
    "}\n",
    "\n",
    "# Запуск агента\n",
    "result = agent.invoke(state)\n",
    "\n",
    "# Вывод результата\n",
    "for entry in result[\"memory\"]:\n",
    "    print(entry)\n",
    "    \n",
    "    \n",
    "state = {\n",
    "    \"messages\": [{\"content\": \"Какое удобрение подойдёт для Кактуса?\"}],\n",
    "    \"memory\": [],\n",
    "    \"db_results\": None,\n",
    "    \"web_results\": None\n",
    "}\n",
    "\n",
    "# Запуск агента\n",
    "result = agent.invoke(state)\n",
    "\n",
    "# Вывод результата\n",
    "for entry in result[\"memory\"]:\n",
    "    print(entry)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализация модели и коллекции\n",
    "model = model_mistral\n",
    "collection = collection_light\n",
    "embedder = embedder_light  \n",
    "\n",
    "# Создание агента\n",
    "agent = Agent(model, collection, embedder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вопрос бд:Какой пост занимает Назарбаев?, Ответ бд:глава Совета безопасности Казахстана, председателя правящей партии «Нур Отан\n",
      "Оценка релевантности:  Нет\n",
      "По поиску в DuckDuckGo:  В настоящее время должность Губернатора Саратовской области занимает Роман Викторович Бусаргин. Его вступление в должность состоялось 16 сентября 2022 года после победы на выборах, прошедших с 9 по 11 сентября того же года. Он получил поддержку 72,55% избирателей (716 974 голоса), став официально зарегистрированным губернатором. Церемония инаугурации прошла в Правительстве области.\n",
      "\n",
      "        Следует отметить, что 21 сентября предыдущего года Валерий Радаев официально вступил в должность Губернатора Саратовской области после назначения Президентом РФ.\n",
      "\n",
      "        Сегодня, 29 июля, Роман Бусаргин отметил свой 43-й день рождения.\n"
     ]
    }
   ],
   "source": [
    "state = {\n",
    "    \"messages\": [],\n",
    "    \"memory\": [],\n",
    "    \"db_results\": None,\n",
    "    \"web_results\": None\n",
    "}\n",
    "state['messages'] = [{\"content\":'Кто губернатор Саратовской области?'}]\n",
    "\n",
    "# Запуск агента\n",
    "result = agent.invoke(state)\n",
    "\n",
    "# Вывод результата\n",
    "print(result[\"memory\"][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тесты с памятью"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Отредактированый вопрос: Какой вид спорта проводится Кубком Стэнли?\n",
      "Ответ бота: По поиску в RuBQ Ответ: Хоккей\n",
      "--------------------------\n",
      "Отредактированый вопрос: Кубок Гагарина - это какой вид спорта?\n",
      "Ответ бота: По поиску в RuBQ Ответ: Хоккей с шайбой\n",
      "--------------------------\n",
      "Отредактированый вопрос: В каком типе спорта разыгрывается Женский Кубок Убера?\n",
      "Ответ бота: По поиску в RuBQ Ответ: в бадминтоне\n"
     ]
    }
   ],
   "source": [
    "# тут в выводах много лишней информации, чтобы посмотреть как работают разные ллм узлы.\n",
    "\n",
    "# Инициализация состояния\n",
    "state = {\n",
    "    \"messages\": [],\n",
    "    \"memory\": [],\n",
    "    \"db_results\": None,\n",
    "    \"web_results\": None\n",
    "}\n",
    "\n",
    "# Первый запрос\n",
    "state['messages'] = [{\"content\": \"В каком виде спорта разыгрывают кубок Стэнли?\"}]\n",
    "result = agent.invoke(state)\n",
    "\n",
    "# Вывод результата первого запроса\n",
    "print(\"Ответ бота:\", result[\"memory\"][-1][7:])\n",
    "\n",
    "print('--------------------------')\n",
    "\n",
    "# Второй запрос (с учётом памяти)\n",
    "state['messages'] = [{\"content\": \"А кубок Гагарина?\"}]\n",
    "result = agent.invoke(state)\n",
    "\n",
    "# Вывод результата второго запроса\n",
    "print(\"Ответ бота:\", result[\"memory\"][-1][7:])\n",
    "\n",
    "print('--------------------------')\n",
    "\n",
    "# Второй запрос (с учётом памяти)\n",
    "state['messages'] = [{\"content\": \"А в каком виде спорта разыгрывают женский кубок Убер??\"}]\n",
    "result = agent.invoke(state)\n",
    "\n",
    "# Вывод результата второго запроса\n",
    "print(\"Ответ бота:\", result[\"memory\"][-1][7:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Итог \n",
    "\n",
    "* Я попробовал 2 совершенно разных подхода с оценкой релевантоности используя расстояния между эмюедингами что было достаточно плохим результатом   \n",
    "  \n",
    "### 1 вариант содержал следующий prompt  \n",
    "   \n",
    "prompt = \"\"\"Ты — интеллектуальный ассистент, специализирующийся на ответах на вопросы на основе базы знаний и поиска в интернете. Твоя задача — предоставлять точные, полезные и структурированные ответы на вопросы пользователя, используя следующие возможности:\n",
    "\n",
    "1. **База знаний**:\n",
    "   - У тебя есть доступ к базе знаний RuBQ 2.0, содержащей вопросы и ответы на русском языке.\n",
    "   - Если вопрос пользователя релевантен базе знаний, используй информацию из неё для формирования ответа.\n",
    "\n",
    "2. **Поиск в интернете**:\n",
    "   - Если информация отсутствует в базе знаний, используй поиск в интернете (DuckDuckGo) для нахождения ответа.\n",
    "   - Обязательно проверяй достоверность найденной информации.\n",
    "\n",
    "3. **Память диалога**:\n",
    "   - Ты должен запоминать контекст диалога с пользователем, чтобы отвечать на последующие вопросы с учётом предыдущих.\n",
    "   - Используй память для уточнения вопросов и предоставления более точных ответов.\n",
    "\n",
    "4. **Формат ответа**:\n",
    "   - Ответ должен быть чётким, структурированным и понятным.\n",
    "   - Если информация найдена в базе знаний, начни ответ с: \"Согласно базе знаний: [ответ]\".\n",
    "   - Если информация найдена в интернете, начни ответ с: \"Согласно результатам поиска: [ответ]\".\n",
    "   - Если ответ требует уточнения, задай уточняющий вопрос пользователю.\n",
    "\n",
    "5. **Работа с инструментами**:\n",
    "   - Для поиска в базе знаний используй инструмент `retrieve_from_db`.\n",
    "   - Для поиска в интернете используй инструмент `DuckDuckGoSearchRun`.\n",
    "   - Если инструмент возвращает ошибку или некорректные данные, сообщи об этом пользователю и попробуй другой подход.\n",
    "\n",
    "6. **Язык и стиль**:\n",
    "   - Отвечай на русском языке.\n",
    "   - Используй вежливый и профессиональный тон.\n",
    "   - Избегай избыточного использования технических терминов, если пользователь не запрашивает их явно.\n",
    "\n",
    "7. **Обработка ошибок**:\n",
    "   - Если ты не можешь найти ответ на вопрос, честно сообщи об этом пользователю и предложи альтернативные варианты (например, уточнить вопрос или поискать в других источниках).\n",
    "\n",
    "8. **Примеры работы**:\n",
    "   - Вопрос: \"Что такое квантовая механика?\"\n",
    "     Ответ: \"Согласно базе знаний: Квантовая механика — это раздел физики, изучающий поведение частиц на атомном и субатомном уровнях.\"\n",
    "   - Вопрос: \"Как ухаживать за алоэ?\"\n",
    "     Ответ: \"Согласно результатам поиска: Алоэ требует умеренного полива, яркого света и хорошо дренированной почвы.\"\n",
    "\n",
    "9. **Важные правила**:\n",
    "   - Никогда не выдумывай информацию. Если ты не уверен в ответе, скажи об этом.\n",
    "   - Всегда проверяй релевантность информации перед тем, как использовать её в ответе.\n",
    "   - Следи за контекстом диалога, чтобы избежать повторений или противоречий.\"\"\"\n",
    "\n",
    "\n",
    "#### с таким вариантом и другим построением графа модель работала очень плохо, почти никигда не обращалась к бд а использовала тольео те данные на которых была обучена.\n",
    "\n",
    "*  Куда лучшей идей было сделать оценивание релевантности также при помощи LLM\n",
    "*  Итоговой модейлью выбрана Mistral 8b так как Llama3.2 3b Очень часто выдаёт плохой результат \n",
    " напрмер при вопросе `\"Какое удабрение подойдет кактусам\"` вопрос `\"Какое растение перерабатывают в текилу\"` она считает одинаковыми\n",
    " Mistral с оценкой справляется куда лучше\n",
    "* Также с задачей отредактировать результат из интернета LLama плохо справляется вставляя пустые символы или используя слова на англйском иногда даже просто пишет транслитом \n",
    "например `Для кактосов mojet podoiti udobrenie naprimer`  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
